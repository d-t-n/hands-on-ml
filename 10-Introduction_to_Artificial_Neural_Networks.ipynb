{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10. Introduction_to_Artificial_Neural_Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial neural networks (ANNs) draw inspiration from the brain's architecture to construct intelligent machines. These networks, while initially conceived as models resembling biological neurons, have evolved considerably from their natural counterparts. The resemblance has even sparked debates about whether to retain the biological analogy in terminology. \n",
    "\n",
    "ANNs, at the heart of Deep Learning, prove highly versatile and scalable, perfectly suited for handling complex tasks such as image classification, speech recognition, recommendation systems, and strategic game playing. This chapter introduces ANNs, tracing their historical development from early architectures to contemporary Multilayer Perceptrons (MLPs) and demonstrates how to implement neural networks using the Keras API, a powerful and expressive tool for building a variety of neural network architectures.\n",
    "\n",
    "The journey of ANNs dates back to 1943, initiated by Warren McCulloch and Walter Pitts, who introduced the first artificial neural network architecture. Despite initial excitement, ANNs faced setbacks and entered periods of dormancy. However, recent advancements have reignited interest in ANNs due to the availability of vast training data, increased computing power, improved training algorithms, and a more nuanced understanding of their limitations. \n",
    "\n",
    "ANNs have begun to demonstrate their potential across various applications, promising a significant impact on our lives driven by a virtuous cycle of funding, progress, and remarkable products. The chapter also provides insights into biological neurons' structure and function, highlighting the foundational inspiration for artificial neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Computations with Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logical computations with artificial neurons, as originally conceptualized by McCulloch and Pitts, are based on a simplified model of biological neurons. These artificial neurons have binary inputs and a binary output, with activation occurring when a predefined number of inputs are active. The process can be summarized in several steps:\n",
    "\n",
    "- Model Overview: Artificial neurons mimic biological neurons with binary (on/off) inputs and outputs.\n",
    "\n",
    "- Universal Computations: McCulloch and Pitts demonstrated that networks of these artificial neurons can compute any logical proposition.\n",
    "\n",
    "- Network Illustrations: Several example networks performing logical computations are presented:\n",
    "\n",
    "   - Identity Function: Neuron C is activated if neuron A is active.\n",
    "\n",
    "   - Logical AND: Neuron C is activated only when both neurons A and B are active.\n",
    "\n",
    "   - Logical OR: Neuron C is activated if either neuron A or neuron B is active, or both.\n",
    "\n",
    "   - Complex Logic (with inhibition): Neuron C is activated only if neuron A is active and neuron B is inactive. This represents a logical NOT operation when neuron A is active all the time.\n",
    "\n",
    "Combinatorial Potential: These simple networks can be combined to compute complex logical expressions, enabling the construction of more intricate computational models.\n",
    "\n",
    "This demonstrates how basic artificial neurons can perform fundamental logical operations, paving the way for constructing more sophisticated neural networks capable of handling complex tasks and computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logica"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
