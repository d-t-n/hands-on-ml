{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. End-to-End Machine Learning Project\n",
    "\n",
    "In this chapter you will work through an example project end to end,\n",
    "pretending to be a recently hired data scientist at a real estate company. Here\n",
    "are the main steps you will go through:\n",
    "1. Look at the big picture.\n",
    "2. Get the data.\n",
    "3. Discover and visualize the data to gain insights.\n",
    "4. Prepare the data for Machine Learning algorithms.\n",
    "5. Select a model and train it.\n",
    "6. Fine-tune your model.\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Real Data\n",
    "\n",
    "When you are learning about Machine Learning, it is best to experiment with\n",
    "real-world data, not artificial datasets. Fortunately, there are thousands of\n",
    "open datasets to choose from, ranging across all sorts of domains. Here are a\n",
    "few places you can look to get data:\n",
    "\n",
    "When we are learning about ML, it's best to work with real data sets, not artificial ones. We list the following data sources:\n",
    "\n",
    "- Popular open data reposatories\n",
    "    - [UC Irvine ML repo](https://archive.ics.uci.edu/ml/index.php)\n",
    "    - [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "    - [Amazon AWS Datasets](https://registry.opendata.aws/)\n",
    "- Meta Portals: they list open data reposatories\n",
    "    - [Data Portals](http://dataportals.org/)\n",
    "    - [OpenDataMonitor](https://opendatamonitor.eu/frontend/web/index.php?r=dashboard%2Findex)\n",
    "    - [Quandl](https://www.quandl.com/)\n",
    "- Other pages listing many open data reposatories\n",
    "    - [Wikipedia](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research)\n",
    "    - [Quora](https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public)\n",
    "    - [The Datasets Subreddit](https://www.reddit.com/r/datasets)\n",
    "\n",
    "In this chapter we’ll use the California Housing Prices dataset from the\n",
    "StatLib repository. This dataset is based on data from the\n",
    "1990 California census. It is not exactly recent (a nice house in the Bay Area\n",
    "was still affordable at the time), but it has many qualities for learning, so we\n",
    "will pretend it is recent data. For teaching purposes I’ve added a categorical\n",
    "attribute and removed a few features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Look at the big picture\n",
    "\n",
    "Our first task is to use the california census data to build a model of the housing prices in the state. This data includes features such as:\n",
    "- Population\n",
    "- Median Income\n",
    "- Median housing price for each block group in California\n",
    "\n",
    "A block group is the smallest geographical unit for which cencus data is published. A Block group has a population between 600 to 3,000. We will call them \"districts\" for short.\n",
    "    \n",
    "Our model should be able to predict the median housing price for any district, given the other features.\n",
    "\n",
    "### Framing the Problem:\n",
    "- The task is to build a model of housing prices in California using census data.\n",
    "- The model should predict the median housing price in any district based on various metrics.\n",
    "- The predicted prices will be used in a downstream Machine Learning system for real estate investments.\n",
    "\n",
    "Pipelines:\n",
    "- Data processing components in a data pipeline are used to manipulate and transform the data.\n",
    "- Components run asynchronously, processing data and passing the results to the next component.\n",
    "- The architecture is robust but requires proper monitoring to avoid performance issues.\n",
    "\n",
    "Current Solution:\n",
    "- The current solution involves manual estimation of district housing prices by experts.\n",
    "- Estimates are often inaccurate, with deviations of more than 20% from the actual prices.\n",
    "- Building a model to predict median housing prices using census data is seen as a cost-effective and accurate alternative.\n",
    "\n",
    "Designing the System:\n",
    "- The problem is framed as a supervised multiple regression task.\n",
    "- The goal is to predict the median housing price using multiple features of the district.\n",
    "- Batch learning is suitable as the data is small enough to fit in memory.\n",
    "\n",
    "### Select a Performance Measure:\n",
    "- The Root Mean Square Error (RMSE) is a typical performance measure for regression tasks.\n",
    "- RMSE quantifies the average prediction error, giving more weight to larger errors.\n",
    "- Mean Absolute Error (MAE) can also be considered in the presence of outliers.\n",
    "\n",
    "### Check the Assumptions:\n",
    "- Assumptions need to be verified to ensure the system aligns with downstream requirements.\n",
    "- In this case, it is confirmed that the downstream system needs actual prices, not categories.\n",
    "- It is crucial to verify assumptions early on to avoid wasting time on the wrong approach.\n",
    "\n",
    "Overall, the task involves building a model to predict housing prices in California using census data. The model will be integrated into a data pipeline for real estate investments. The current manual estimation process is costly and inaccurate, leading to the need for a more efficient and accurate solution. The system will be designed as a supervised multiple regression task using batch learning techniques. Performance will be evaluated using the RMSE measure. Assumptions about the downstream system's requirements have been verified to ensure the system's alignment with expectations.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get the Data\n",
    "\n",
    "### Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"end_to_end_project\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to fetch data\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
