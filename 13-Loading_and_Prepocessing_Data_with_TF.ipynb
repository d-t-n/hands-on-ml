{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13. Loading and Prepocessing Data with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, the author emphasizes the challenges of training deep learning systems on large datasets that surpass available memory capacity. \n",
    "The focus is on the challenges of handling large datasets in deep learning systems, specifically those that cannot fit into memory. TensorFlow addresses this issue through its Data API, simplifying the process of ingesting and preprocessing vast datasets. Key points include:\n",
    "\n",
    "- Data API in TensorFlow: Facilitates efficient handling of large datasets by creating a dataset object, specifying data sources, and transformation methods, with TensorFlow managing implementation details like multithreading, queuing, batching, and prefetching.\n",
    "- Compatibility with tf.keras: The Data API seamlessly integrates with tf.keras, providing a smooth workflow for deep learning model development.\n",
    "- Supported data formats: TensorFlow's Data API can read from various sources, including text files (CSV), binary files (fixed-size records and TFRecord format), and SQL databases, enhancing flexibility in data handling.\n",
    "- Preprocessing challenges: Efficient preprocessing of diverse data, including normalization and encoding of text, categorical features, etc., is crucial, with options like custom preprocessing layers or using standard Keras preprocessing layers.\n",
    "- Related TensorFlow projects: The discussion briefly introduces TF Transform, enabling batch-mode preprocessing for training sets, and TF Datasets, offering a convenient function for downloading and manipulating common datasets through the Data API. These tools contribute to a comprehensive ecosystem for effective deep learning workflows in TensorFlow.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Data API Overview\n",
    "\n",
    "The Data API in TensorFlow revolves around the concept of a dataset, representing a sequence of data items. Key functionalities include:\n",
    "\n",
    "- **Creating a Dataset:** Use `tf.data.Dataset.from_tensor_slices()` to create a dataset in RAM, providing a tensor as input. The dataset consists of slices of the tensor along its first dimension.\n",
    "\n",
    "- **Iterating Over a Dataset:** Easily iterate over dataset items using a simple loop. Each item is a tensor representing a slice of the original data.\n",
    "\n",
    "- **Chaining Transformations:** Apply various transformations to a dataset using methods like `repeat()` and `batch()`. Chaining transformations allows for efficient preprocessing and handling of large datasets.\n",
    "\n",
    "- **Data Shuffling:** Use the `shuffle()` method to shuffle instances in the training set, improving the independence and identically distributed nature required for Gradient Descent.\n",
    "\n",
    "- **Interleaving Lines from Multiple Files:** Demonstrated using the `interleave()` method, allowing for the interleaving of lines from multiple files for efficient data shuffling.\n",
    "\n",
    "- **Prefetching:** Enhance performance by prefetching the next batch of data while the current batch is being processed, ensuring better CPU and GPU utilization.\n",
    "\n",
    "- **Preprocessing the Data:** Implement a preprocessing function using TensorFlow functions like `decode_csv()` and `stack()` to parse, scale, and preprocess data items.\n",
    "\n",
    "- **Building a Reusable Input Pipeline:** Create a function `csv_reader_dataset()` that efficiently loads, preprocesses, shuffles, repeats, batches, and prefetches data from multiple CSV files.\n",
    "\n",
    "- **Using the Dataset with tf.keras:** Integrate the created dataset with the Keras API for model training, evaluation, and prediction. Pass datasets directly to `fit()`, `evaluate()`, and `predict()` methods, simplifying the model training process.\n",
    "\n",
    "- **Custom Training Loop:** Optionally, iterate over the dataset manually for a custom training loop. This offers flexibility in building advanced training procedures.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFRecord Format and Compression\n",
    "\n",
    "The TFRecord format is TensorFlow's preferred way of efficiently storing and reading large amounts of data. It is a binary format consisting of records with length, CRC checksums, and actual data. Creating a TFRecord file is easy using `tf.io.TFRecordWriter`, and reading it can be done with `tf.data.TFRecordDataset`. Compression, especially for network loading, can be achieved using options like GZIP.\n",
    "\n",
    "```python\n",
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")\n",
    "\n",
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol Buffers (protobuf)\n",
    "\n",
    "TFRecord files often contain serialized protocol buffers (protobufs). Protobufs are a binary format developed by Google, now widely used, and can be defined using a simple language. TensorFlow provides protobuf definitions, like `tf.train.Example`, commonly used in TFRecord files.\n",
    "\n",
    "```python\n",
    "from tensorflow.train import BytesList, FloatList, Int64List, Feature, Features, Example\n",
    "\n",
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
    "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
    "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
    "        }\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Parsing Examples\n",
    "\n",
    "Loading and parsing serialized Examples from a TFRecord file is done using `tf.data.TFRecordDataset` and `tf.io.parse_single_example()`. Features are described using a dictionary, and parsing involves specifying the types and shapes of the features.\n",
    "\n",
    "```python\n",
    "feature_description = {\n",
    "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "\n",
    "for serialized_example in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]):\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "    # ... (processing the parsed example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SequenceExample for Lists of Lists\n",
    "For more complex structures, like lists of lists, TensorFlow provides SequenceExample. It contains context features and feature lists, where each feature can be a list of byte strings, a list of 64-bit integers, or a list of floats.\n",
    "\n",
    "```python\n",
    "from tensorflow.train import FeatureList, FeatureLists, SequenceExample\n",
    "\n",
    "sequence_example = SequenceExample(\n",
    "    context=Features(feature={\"author\": Feature(bytes_list=BytesList(value=[b\"John\"]))}),\n",
    "    feature_lists=FeatureLists(\n",
    "        feature_list={\n",
    "            \"content\": FeatureList(feature=[Feature(bytes_list=BytesList(value=[b\"word1\", b\"word2\"]))]),\n",
    "            \"comments\": FeatureList(feature=[Feature(bytes_list=BytesList(value=[b\"comment1\", b\"comment2\"]))])\n",
    "        }\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
