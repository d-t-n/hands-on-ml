# Hands-on ML
 Example code and solutions to the exercises in my O'Reilly book Hands-on Machine Learning with Scikit-Learn and TensorFlow.

 Book Content Index

## Part I. The Fundamentals of Machine Learning
1. The Machine Learning Landscape
    - What Is Machine Learning?
    - Why Use Machine Learning?
    - Types of Machine Learning Systems
    - Supervised/Unsupervised Learning
    - Batch and Online Learning
    - Instance-Based Versus Model-Based Learning 
    - Main Challenges of Machine Learning
    - Insufficient Quantity of Training Data
    - Nonrepresentative Training Data
    - Poor-Quality Data
    - Irrelevant Features
    - Overfitting the Training Data
    - Underfitting the Training Data
    - Stepping Back
    - Testing and Validating
    - Hyperparameter Tuning and Model Selection
    - Data Mismatch

        Notebook:
        - [The Machine Learning Landscape](https://nbviewer.jupyter.org/github/d-t-n/d-t-n/blob/master/01-The_ML_Landscape.ipynb)
<br />
<br />
<br /> 
2. End-to-end Machine Learnng Project
    - Look at the big picture
    - Get the data
    - Discover and visualize the data to gain insights
    - Prepare the data for Machine Learning algorithms
    - Select a model and train it
    - Fine-tune your model
    - Present your solution
    - Launch, monitor, and maintain your system

        Notebook:
        - [End-to-end Machine Learnng Project](https://nbviewer.jupyter.org/github/d-t-n/d-t-n/blob/master/02-End_to_End_Machine_Learning_Project-.ipynb)
<br />
<br />
<br />
3. Classification
    - MNIST
    - Training a Binary Classifier
    - Performance Measures
    - Multiclass Classification
    - Error Analysis
    - Multilabel Classification
    - Multioutput Classification

        Notebook:
        - [Classification](https://nbviewer.jupyter.org/github/d-t-n/d-t-n/blob/master/03-Classification.ipynb)
<br />
<br />
<br />
4. Training Models
    - Linear Regression
    - Gradient Descent
    - Polynomial Regression
    - Learning Curves
    - Regularized Linear Models
    - Logistic Regression

        Notebook:
        - [Training Models](https://nbviewer.jupyter.org/github/d-t-n/d-t-n/blob/master/04-Training_Models.ipynb)
<br />
<br />
<br />
5. Support Vector Machines
    - Linear SVM Classification
    - Nonlinear SVM Classification
    - SVM Regression
    - Under the Hood

        Notebook:
        - [Support Vector Machines](https://nbviewer.jupyter.org/github/d-t-n/d-t-n/blob/master/05-Support_Vector_Machines.ipynb)
<br />
<br />
<br />
6. Decision Trees
    - Training and Visualizing a Decision Tree
    - Making Predictions
    - Estimating Class Probabilities
    - The CART Training Algorithm
    - Computational Complexity
    - Gini Impurity or Entropy?
    - Regularization
    - Regression
    - Instability

        Notebook:
        - [Decision Trees](https://nbviewer.jupyter.org/github/d-t-n/d-t-n/blob/master/06-Decision_Trees.ipynb)
<br />
<br />
<br />
7. Ensemble Learning and Random Forests
    - Voting Classifiers
    - Bagging and Pasting
    - Random Patches and Random Subspaces
    - Random Forests
    - Boosting
    - Stacking

        Notebook:
        - [Ensemble Learning and Random Forests](https://nbviewer.jupyter.org/github/d-t-n/d-t-n/blob/master/07-Ensemble_Models.ipynb)
<br />
<br />
<br />
8. Dimensionality Reduction
    - The Curse of Dimensionality

        Notebook:
        - [Dimensionality Reduction](https://nbviewer.jupyter.org/github/d-t-n/d-t-n/blob/master/08-Dimensionality_Reduction.ipynb)
<br />
<br />
<br />
9. Unsupervised Learning Techniques
    - Clustering
    - Gaussian Mixtures

        Notebook:
        - [Unsupervised Learning Techniques](https://nbviewer.jupyter.org/github/d-t-n/d-t-n/blob/master/09-Unsupervised_Learning_Techniques.ipynb)
<br />
<br />
<br />
10. Introduction to Artificial Neural Networks with Keras
    - From Biological to Artificial Neurons
    - Implementing MLPs with Keras
    - Fine-Tuning Neural Network Hyperparameters

        Notebook:
        - [Unsupervised Learning Techniques](https://nbviewer.jupyter.org/github/d-t-n/d-t-n/blob/master/10-Introduction_to_Artificial_Neural_Networks.ipynb)
<br />
<br />
<br />
11. Training Deep Neural Networks
    - The Vanishing/Exploding Gradients Problems
    - Reusing Pretrained Layers
    - Faster Optimizers
    - Avoiding Overfitting Through Regularization

        Notebook:
        - [Training Deep Neural Networks](https://nbviewer.jupyter.org/github/d-t-n/d-t-n/blob/master/11-Training_Deep_Neural_Networks.ipynb)